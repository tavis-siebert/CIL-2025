{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Plots for the poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import setup_logging\n",
    "from utils_ext.ml import load_wandb_history, load_wandb_summary\n",
    "from utils_ext.plot import Plotter\n",
    "from utils_ext.tools import parse_logs\n",
    "from cache import CACHE\n",
    "\n",
    "plt.ioff()\n",
    "setup_logging()\n",
    "\n",
    "PATH_DATA = Path(\"../data\")\n",
    "PATH_OUTPUT = Path(\"../output/results\")\n",
    "CACHE.init(cache_dir=\"../output/cache\")\n",
    "\n",
    "# setup plotter\n",
    "\n",
    "FONTSIZE_SMALL = 6\n",
    "FONTSIZE_DEFAULT = 8\n",
    "FONTSIZE_LARGE = 10\n",
    "\n",
    "Plotter.setup(\n",
    "    css_patches=[\"overflow_auto\", \"gray_background\"]\n",
    ")\n",
    "Plotter.configure(\n",
    "    basewidth=3.25,\n",
    "    fontsize=FONTSIZE_DEFAULT,\n",
    "    latex=False,\n",
    "    rcparams={\n",
    "        \"lines.linewidth\": 1,  # default: 1.5\n",
    "        \"axes.labelpad\": 2,  # default: 4\n",
    "    },\n",
    "    save_dir=PATH_OUTPUT / \"plots_poster\",\n",
    "    save_format=\"pdf\",\n",
    ")\n",
    "Plotter.configure(\n",
    "    latex=True,\n",
    "    latex_preamble=\"\\n\".join(\n",
    "        [\n",
    "            r\"\\usepackage[utf8]{inputenc}\",\n",
    "            r\"\\usepackage[T1]{fontenc}\",\n",
    "            r\"\\usepackage{microtype}\",\n",
    "            r\"\\usepackage{lmodern}\",  # for 8-bit Latin Modern font\n",
    "            r\"\\usepackage[sc]{mathpazo}\",  # for Palatino font\n",
    "            r\"\\usepackage{amsmath,amssymb,amsfonts,mathrsfs}\",\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_MAPPING = {\n",
    "#     \"cardiffnlp/twitter-roberta-base-sentiment-latest\": \"roberta-b-cardiffnlp\",\n",
    "#     \"tabularisai/multilingual-sentiment-analysis\": \"distilbert-b-tabularisai\",\n",
    "#     \"siebert/sentiment-roberta-large-english\": \"roberta-l-siebert\",\n",
    "#     \"FacebookAI/roberta-large\": \"roberta-l\",\n",
    "# }\n",
    "DESCRIPTION_MAPPING = {\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\": \"RoBERTa-B (CardiffNLP)\",\n",
    "    \"tabularisai/multilingual-sentiment-analysis\": \"DistilBERT-B (tabularisai)\",\n",
    "    \"siebert/sentiment-roberta-large-english\": \"RoBERTa-L (SiEBERT)\",\n",
    "    \"FacebookAI/roberta-large\": \"RoBERTa-L\",\n",
    "}\n",
    "\n",
    "def annotate_ours(ax, x, y=0.25):\n",
    "    ax.axvline(x=x, color=\"tab:gray\", linestyle=\"--\", alpha=0.5)\n",
    "    x_axes = ax.transAxes.inverted().transform(ax.transData.transform((x, 0)))[0]\n",
    "    ax.text(\n",
    "        x_axes-0.04, y, \"ours\",\n",
    "        fontsize=FONTSIZE_SMALL,\n",
    "        color=\"tab:gray\",\n",
    "        rotation=90,\n",
    "        # clip_on=False,\n",
    "        transform=ax.transAxes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Main plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.configure(save_always=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_finetuned_classifier_class_weights(plot_group):\n",
    "    def format_class_weights(x):\n",
    "        if pd.isna(x):\n",
    "            return \"none\"\n",
    "        elif x == \"auto\":\n",
    "            return \"balanced\"\n",
    "        elif x == \"[1.553279,1.038669,1.096438]\":\n",
    "            return \"adjusted\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown class weights: {x}\")\n",
    "\n",
    "    # load data\n",
    "    df = load_wandb_summary(\n",
    "        PATH_OUTPUT / \"data\" / \"finetuned_classifier_class_weights.csv\",\n",
    "        {\n",
    "            \"train_score\": \"train_score\",\n",
    "            \"val_score\": \"val_score\",\n",
    "            \"pipeline.trainer.class_weights\": \"class_weights\",\n",
    "        },\n",
    "    )\n",
    "    df[\"class_weights\"] = df[\"class_weights\"].apply(format_class_weights)\n",
    "\n",
    "    # add test scores\n",
    "    test_scores = pd.Series({\n",
    "        \"20250522-231950-polite-boar-341\": 0.85694,\n",
    "        \"20250522-232111-sneaky-sponge-426\": 0.85906,\n",
    "        \"20250522-232140-agreeable-auk-109\": 0.85491,\n",
    "    })\n",
    "    df[\"test_score\"] = test_scores\n",
    "\n",
    "    # update dataframe index and order\n",
    "    df = df.set_index(\"class_weights\")\n",
    "    df = df.reindex([\"none\", \"balanced\", \"adjusted\"])\n",
    "\n",
    "    # plot\n",
    "    fig, ax = Plotter.create()\n",
    "    df[\"train_score\"].plot.bar(\n",
    "        ax=ax,\n",
    "        color=\"tab:blue\",\n",
    "        xlabel=\"class weights\",\n",
    "        ylabel=\"train score\",\n",
    "        ylim=(0.905, 0.955),\n",
    "        yticks=[0.91, 0.93, 0.95],\n",
    "    )\n",
    "    ax.set_xticklabels(df.index, rotation=35, ha=\"right\")\n",
    "    plot_group.add_plot(fig, \"finetuned_classifier-class_weights-train\")\n",
    "\n",
    "    fig, ax = Plotter.create()\n",
    "    df[\"val_score\"].plot.bar(\n",
    "        ax=ax,\n",
    "        color=\"tab:orange\",\n",
    "        xlabel=\"class weights\",\n",
    "        ylabel=\"validation score\",\n",
    "        ylim=(0.845, 0.895),\n",
    "        yticks=[0.85, 0.87, 0.89],\n",
    "    )\n",
    "    ax.set_xticklabels(df.index, rotation=35, ha=\"right\")\n",
    "    plot_group.add_plot(fig, \"finetuned_classifier-class_weights-val\")\n",
    "\n",
    "    fig, ax = Plotter.create()\n",
    "    df[\"test_score\"].plot.bar(\n",
    "        ax=ax,\n",
    "        color=\"tab:green\",\n",
    "        xlabel=\"class weights\",\n",
    "        ylabel=\"(public) test score\",\n",
    "        ylim=(0.845, 0.895),\n",
    "        yticks=[0.85, 0.87, 0.89],\n",
    "    )\n",
    "    ax.set_xticklabels(df.index, rotation=35, ha=\"right\")\n",
    "    plot_group.add_plot(fig, \"finetuned_classifier-class_weights-test\")\n",
    "\n",
    "    print(df.reset_index().to_latex(\n",
    "        index=False,\n",
    "        header=[\"class weights\", 'train score', 'val score', '(public) test score'],\n",
    "        float_format=\"%.3f\",\n",
    "    ))\n",
    "\n",
    "\n",
    "with Plotter.group(\n",
    "    figwidth=0.75,\n",
    "    grid_ncols=3,\n",
    "    consistent_size=True,\n",
    "    save_kw=dict(transparent=True),\n",
    "    # save=True,\n",
    ") as plot_group:\n",
    "    plot_finetuned_classifier_class_weights(plot_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_finetuned_classifier_loss(plot_group):\n",
    "    # load data\n",
    "    df = load_wandb_history(\n",
    "        PATH_OUTPUT / \"data\" / \"finetuned_classifier_loss.csv\",\n",
    "        {\n",
    "            \"train/epoch\": \"epoch\",\n",
    "            \"train/loss\": \"train_loss\",\n",
    "            \"eval/loss\": \"val_loss\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    df_by_model = {\n",
    "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\": df[\"20250523-172805-illustrious-horse-512\"],\n",
    "        \"tabularisai/multilingual-sentiment-analysis\": pd.concat([\n",
    "            df[\"20250527-203545-peaceful-wren-73\"],\n",
    "            df[\"20250528-085739-calm-shrike-582\"],\n",
    "        ]),\n",
    "        \"siebert/sentiment-roberta-large-english\": pd.concat([\n",
    "            df[\"20250528-084638-polite-sloth-517\"],\n",
    "            df[\"20250529-011210-bouncy-cat-232\"],\n",
    "            df[\"20250529-171132-inquisitive-stoat-571\"],\n",
    "        ]),\n",
    "        \"FacebookAI/roberta-large\": pd.concat([\n",
    "            df[\"20250528-203625-enthused-kite-698\"],\n",
    "            df[\"20250528-235716-wistful-boar-764\"],\n",
    "            df[\"20250528-235901-legendary-crab-346\"],\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    for name, df_model in df_by_model.items():\n",
    "        # smooth train loss\n",
    "        df_model[\"train_loss_ema\"] = df_model[\"train_loss\"].ewm(alpha=0.05).mean()\n",
    "\n",
    "        # plot\n",
    "        fig, ax = Plotter.create()\n",
    "        df_model[[\"epoch\", \"train_loss_ema\"]].dropna().plot.line(\n",
    "            ax=ax,\n",
    "            x=\"epoch\",\n",
    "            y=\"train_loss_ema\",\n",
    "            color=\"tab:blue\",\n",
    "            ylabel=\"loss\",\n",
    "        )\n",
    "        df_model[[\"epoch\", \"val_loss\"]].dropna().plot.line(\n",
    "            ax=ax,\n",
    "            x=\"epoch\",\n",
    "            y=\"val_loss\",\n",
    "            color=\"tab:orange\",\n",
    "        )\n",
    "        df_model[[\"epoch\", \"train_loss\"]].dropna().plot.line(\n",
    "            ax=ax,\n",
    "            x=\"epoch\",\n",
    "            y=\"train_loss\",\n",
    "            color=\"tab:blue\",\n",
    "            alpha=0.25,\n",
    "            ylabel=\"loss\",\n",
    "        )\n",
    "        # ax.set_title(DESCRIPTION_MAPPING[name])\n",
    "        ax.legend([\"train loss\", \"validation loss\"], loc=\"lower left\", fontsize=FONTSIZE_SMALL)\n",
    "        annotate_ours(ax, x=2, y=0.05)\n",
    "        plot_group.add_plot(fig, f\"finetuned_classifier-loss-{name.replace('/', '_').replace('-', '_')}\")\n",
    "\n",
    "\n",
    "with Plotter.group(\n",
    "    figwidth=0.8,\n",
    "    grid_ncols=2,\n",
    "    consistent_size=True,\n",
    "    save_kw=dict(transparent=True),\n",
    "    # save=True,\n",
    ") as plot_group:\n",
    "    plot_finetuned_classifier_loss(plot_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_finetuned_classifier_freeze(plot_group):\n",
    "    def format_layer(x):\n",
    "        if pd.isna(x):\n",
    "            return \"full\"\n",
    "        else:\n",
    "            last_layer = eval(x)[-1]\n",
    "            if last_layer.endswith(\"embeddings\"):\n",
    "                return \"embeddings\"\n",
    "            else:\n",
    "                return \"layer \" + last_layer.split(\".\")[-1]\n",
    "\n",
    "    # load data\n",
    "    df = load_wandb_summary(\n",
    "        PATH_OUTPUT / \"data\" / \"finetuned_classifier_freeze.csv\",\n",
    "        {\n",
    "            \"train_score\": \"train_score\",\n",
    "            \"val_score\": \"val_score\",\n",
    "            \"pipeline.freeze\": \"freeze\",\n",
    "            \"Runtime\": \"train_time\",\n",
    "        },\n",
    "    )\n",
    "    df[\"freeze\"] = df[\"freeze\"].apply(format_layer)\n",
    "    df[\"train_time\"] = df[\"train_time\"] / 3600\n",
    "\n",
    "    # parse number of trainable parameters from log files\n",
    "    logs = parse_logs(\n",
    "        \"../output/experiments_freeze/*/job-*.log\",\n",
    "        patterns={\n",
    "            \"name\": (r\"Job name: (.+)\", None),\n",
    "            \"n_trainable_params\": (r\"Number of trainable parameters: ([\\d,]+)\", lambda x: int(x.replace(\",\", \"\"))),\n",
    "        },\n",
    "    )\n",
    "    logs = pd.DataFrame(logs)\n",
    "    logs = logs.set_index(\"name\")\n",
    "    df[\"n_trainable_params\"] = logs[\"n_trainable_params\"]\n",
    "\n",
    "    # update dataframe index and order\n",
    "    df = df.set_index(\"freeze\")\n",
    "    df = df.loc[::-1]\n",
    "\n",
    "    # plot\n",
    "    fig, ax = Plotter.create()\n",
    "    ax2 = ax.twinx()\n",
    "    df.plot.line(\n",
    "        ax=ax,\n",
    "        y=[\"train_score\", \"val_score\"],\n",
    "        marker=\"o\",\n",
    "        markersize=3,\n",
    "        color={\"train_score\": \"tab:blue\", \"val_score\": \"tab:orange\"},\n",
    "        ylabel=\"score\",\n",
    "    )\n",
    "    Plotter.set(\n",
    "        ax,\n",
    "        legend=dict(loc=\"upper left\", labels=[\"train score\", \"validation score\"], fontsize=FONTSIZE_SMALL),\n",
    "    )\n",
    "    df.plot.line(\n",
    "        ax=ax2,\n",
    "        y=\"train_time\",\n",
    "        marker=\"o\",\n",
    "        markersize=3,\n",
    "        linestyle=\"--\",\n",
    "        color=\"tab:gray\",\n",
    "        ylabel=\"time (h)\",\n",
    "    )\n",
    "    ax.set_xlabel(\"parameters frozen up to\")\n",
    "    Plotter.set(\n",
    "        ax2,\n",
    "        legend=dict(loc=\"lower right\", labels=[\"train time\"], fontsize=FONTSIZE_SMALL),\n",
    "    )\n",
    "    ax.set_xticks(ticks=range(len(df.index))[1::2], labels=df.index[1::2], rotation=35, ha=\"right\")\n",
    "    ax.axvline(x=7, color=\"tab:green\", linestyle=\"--\", alpha=0.5)\n",
    "    annotate_ours(ax, x=13)\n",
    "    plot_group.add_plot(fig, \"finetuned_classifier-freeze\")\n",
    "\n",
    "\n",
    "with Plotter.group(\n",
    "    figwidth=0.85,\n",
    "    grid_ncols=2,\n",
    "    consistent_size=True,\n",
    "    save_kw=dict(transparent=True),\n",
    "    # save=True,\n",
    ") as plot_group:\n",
    "    plot_finetuned_classifier_freeze(plot_group)\n",
    "    plot_finetuned_classifier_loss(plot_group)\n",
    "    del plot_group.plots[-3:]\n",
    "    plot_group.plots[-1] = (plot_group.plots[-1][0], \"finetuned_classifier-loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_finetuned_classifier_post_on_hard(plot_group):\n",
    "    df = load_wandb_summary(\n",
    "        PATH_OUTPUT / \"data\" / \"finetuned_classifier_post_on_hard.csv\",\n",
    "        {\n",
    "            \"train_score\": \"train_score\",\n",
    "            \"val_score\": \"val_score\",\n",
    "            \"eval/loss\": \"eval_loss\",\n",
    "            \"pipeline.preprocessing.difficulty_filter.p\": \"p\",\n",
    "            \"train_runtime\": \"train_time\",\n",
    "            \"_wandb.runtime\": \"wandb_time\",\n",
    "        },\n",
    "    )\n",
    "    df[\"p\"] = df[\"p\"].apply(lambda x: x if not pd.isna(x) else \"none\")\n",
    "    df.loc[\"20250529-164632-charming-stoat-395\", \"train_time\"] = df.loc[\"20250529-164632-charming-stoat-395\", \"wandb_time\"] - 285   # this run was interrupted\n",
    "    df[\"train_time\"] = df[\"train_time\"] / 3600  # convert to hours\n",
    "    df = df.set_index(\"p\")\n",
    "    df = df.loc[[\"none\", 0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "\n",
    "    fig, ax = Plotter.create()\n",
    "    ax2 = ax.twinx()\n",
    "    df.plot.line(\n",
    "        ax=ax,\n",
    "        y=[\"train_score\", \"val_score\"],\n",
    "        # y=[\"val_score\"],\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "        color={\"train_score\": \"tab:blue\", \"val_score\": \"tab:orange\"},\n",
    "        ylabel=\"score\",\n",
    "    )\n",
    "    Plotter.set(\n",
    "        ax,\n",
    "        legend=dict(loc=\"upper left\", labels=[\"train score\", \"validation score\"], fontsize=FONTSIZE_SMALL),\n",
    "        # legend=dict(loc=\"upper left\", labels=[\"validation score\"], fontsize=FONTSIZE_SMALL),\n",
    "    )\n",
    "    df.plot.line(\n",
    "        ax=ax2,\n",
    "        y=\"eval_loss\",\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "        linestyle=\"--\",\n",
    "        color=\"tab:orange\",\n",
    "        ylabel=\"loss\",\n",
    "    )\n",
    "    Plotter.set(\n",
    "        ax2,\n",
    "        legend=dict(loc=\"lower right\", labels=[\"validation loss\"], fontsize=FONTSIZE_SMALL),\n",
    "    )\n",
    "    ax.set_xlabel(\"fraction of hard samples\")\n",
    "    annotate_ours(ax, x=3.5)\n",
    "    plot_group.add_plot(fig, \"finetuned_classifier-post_on_hard\")\n",
    "\n",
    "\n",
    "with Plotter.group(\n",
    "    figwidth=1,\n",
    "    grid_ncols=2,\n",
    "    consistent_size=True,\n",
    "    save_kw=dict(transparent=True),\n",
    "    # save=True,\n",
    ") as plot_group:\n",
    "    plot_finetuned_classifier_post_on_hard(plot_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_score, load_data\n",
    "from cache import load_embeddings\n",
    "from pipelines.pretrained_classifier import map_to_labels\n",
    "\n",
    "\n",
    "def plot_model_voting(plot_group):\n",
    "    model_names = [\n",
    "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "        \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        \"siebert/sentiment-roberta-large-english\",\n",
    "        \"tabularisai/multilingual-sentiment-analysis\",\n",
    "    ]\n",
    "\n",
    "    def load_predictions(model_name):\n",
    "        return load_embeddings(\"huggingface\", model_name, \"predictions_train.csv\", load_kwargs={\"index_col\": 0}, verbose=False)\n",
    "\n",
    "    def load_labels(model_name, label_mapping=None):\n",
    "        predictions = load_predictions(model_name)\n",
    "        labels = map_to_labels(predictions, model_name)\n",
    "        if label_mapping is not None:\n",
    "            labels = labels.map(label_mapping)\n",
    "        return labels\n",
    "\n",
    "    def load_labels_all(model_names, label_mapping=None):\n",
    "        label_pred = {}\n",
    "        for model_name in model_names:\n",
    "            label_pred[model_name] = load_labels(model_name, label_mapping)\n",
    "        return pd.DataFrame(label_pred)\n",
    "\n",
    "    label_mapping = {\n",
    "        \"negative\": -1,\n",
    "        \"neutral\": 0,\n",
    "        \"positive\": 1,\n",
    "    }\n",
    "    train_dataset = load_data(PATH_DATA / \"training.csv\")\n",
    "\n",
    "    # load predictions\n",
    "    sentences = train_dataset[\"sentence\"]\n",
    "    label_true = train_dataset[\"label\"]\n",
    "    label_pred = load_labels_all(model_names)\n",
    "\n",
    "    # count number of votes for each sample\n",
    "    model_voting = label_pred.apply(pd.Series.value_counts, axis=1).fillna(0).astype(int)\n",
    "\n",
    "    # compute score weighted by votes for each sample\n",
    "    scores_individual = 0.5 * (2 - np.abs(model_voting.columns.map(label_mapping).values - label_true.map(label_mapping).values[:, None]))\n",
    "    scores = (model_voting * scores_individual).sum(axis=1) / model_voting.sum(axis=1)\n",
    "\n",
    "    # finalize dataframe\n",
    "    # label_pred_count.rename(columns=label_mapping_inverted, inplace=True)\n",
    "    model_voting[\"label\"] = label_true\n",
    "    model_voting[\"score\"] = scores\n",
    "    model_voting[\"sentence\"] = sentences\n",
    "\n",
    "    # plot distribution of weighted scores\n",
    "    fig, ax = Plotter.create()\n",
    "    model_voting[\"score\"].value_counts().sort_index().plot.bar(ax)\n",
    "    ax.set_xlabel(\"difficulty score\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    plot_group.add_plot(fig, \"finetuned_classifier-difficulty_scores\")\n",
    "\n",
    "    # compute scores per model per score\n",
    "    scores_per_bin = {}\n",
    "    for score in np.sort(model_voting[\"score\"].unique()):\n",
    "        label_true_bin = label_true[model_voting[\"score\"] == score]\n",
    "        label_pred_bin = label_pred[model_voting[\"score\"] == score]\n",
    "        scores_per_bin[score] = [evaluate_score(label_true_bin, label_pred_bin[model_name]) for model_name in model_names]\n",
    "\n",
    "with Plotter.group(\n",
    "    figwidth=0.6,\n",
    "    grid_ncols=1,\n",
    "    consistent_size=True,\n",
    "    save_kw=dict(transparent=True),\n",
    "    # save=True,\n",
    ") as plot_group:\n",
    "    plot_model_voting(plot_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_finetuned_classifier_final():\n",
    "    def format_name(x):\n",
    "        if x.startswith(\"/\"):\n",
    "            return df.loc[x.split(\"/\")[-2], \"model\"]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    # load data\n",
    "    df = load_wandb_summary(\n",
    "        PATH_OUTPUT / \"data\" / \"finetuned_classifier_final.csv\",\n",
    "        {\n",
    "            \"pipeline.model.pretrained_model_name_or_path\": \"model\",\n",
    "            \"pipeline.preprocessing.sanitizer\": \"preprocessing\",\n",
    "            \"pipeline.preprocessing.difficulty_filter.p\": \"post on hard\",\n",
    "            \"Runtime\": \"train_time\",\n",
    "            \"train_score\": \"train_score\",\n",
    "            \"val_score\": \"val_score\",\n",
    "        },\n",
    "    )\n",
    "    df[\"model\"] = df[\"model\"].apply(format_name)\n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(lambda x: \"without\" if pd.isna(x) else \"with\")\n",
    "    df[\"post on hard\"] = df[\"post on hard\"].apply(lambda x: \"without\" if pd.isna(x) else \"with\")\n",
    "    # create multi-index table\n",
    "    df = df.pivot(index=[\"model\"], columns=[\"preprocessing\", \"post on hard\"], values=\"val_score\")\n",
    "    df = df.reindex(\n",
    "        index=[\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            \"tabularisai/multilingual-sentiment-analysis\",\n",
    "            \"siebert/sentiment-roberta-large-english\",\n",
    "            \"FacebookAI/roberta-large\",\n",
    "        ],\n",
    "        columns=pd.MultiIndex.from_product(\n",
    "            [[\"without\", \"with\"], [\"without\", \"with\"]],\n",
    "            names=[\"preprocessing\", \"post on hard\"],\n",
    "        ),\n",
    "    )\n",
    "    # construct final table\n",
    "    df_final = pd.DataFrame({\n",
    "        \"pretrained\": {\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment-latest\": 0.829226,\n",
    "            \"tabularisai/multilingual-sentiment-analysis\": 0.737220,\n",
    "            \"siebert/sentiment-roberta-large-english\": 0.676876,\n",
    "            \"FacebookAI/roberta-large\": None,\n",
    "        },\n",
    "        \"+ finetuned\": df[(\"without\", \"without\")],\n",
    "        \"+ post-tuned\": df[(\"without\", \"with\")],\n",
    "        \"+ preprocessing\": df[(\"with\", \"with\")],\n",
    "    })\n",
    "    # map model names\n",
    "    df.index = df.index.map(DESCRIPTION_MAPPING)\n",
    "    df_final.index = df_final.index.map(DESCRIPTION_MAPPING)\n",
    "\n",
    "\n",
    "    # def color_cell(val, style=\"latex\"):\n",
    "    #     # normalize between min and max\n",
    "    #     min_val = df.min().min()\n",
    "    #     max_val = df.max().max()\n",
    "    #     norm = (val - min_val) / (max_val - min_val) if max_val > min_val else 0\n",
    "    #     # map to color between white (FFFFFF) and blue (0066FF)\n",
    "    #     def interp(a, b):\n",
    "    #         return int(a + (b - a) * norm)\n",
    "    #     r = interp(200, 0)\n",
    "    #     g = interp(200, 102)\n",
    "    #     b = interp(255, 255)\n",
    "    #     color = f\"{r:02X}{g:02X}{b:02X}\"\n",
    "    #     if style == \"latex\":\n",
    "    #         return f'\\\\cellcolor[HTML]{{{color}}} {val:.6f}'\n",
    "    #     else:\n",
    "    #         return f'background-color: #{color}; color: black;'\n",
    "\n",
    "    display(df)\n",
    "    print(df_final.to_latex(\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        float_format=\"%.3f\"\n",
    "    ))\n",
    "    # display(df.style.map(color_cell, style=\"html\"))\n",
    "    # print(df.map(color_cell, style=\"latex\").to_latex(\n",
    "    #     multirow=True,\n",
    "    #     multicolumn=True,\n",
    "    #     escape=False,\n",
    "    #     float_format=\"%.3f\"\n",
    "    # ))\n",
    "plot_finetuned_classifier_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.configure(save_always=False)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

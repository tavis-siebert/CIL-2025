seed: 0
val_split_size: 0.1
deterministic: true

pipeline:
  name: mixture_of_experts.MoEModel

  mode: classification

  # === Define your experts here ===
  # args
  #  - embed_model = type of model used (e.g. microsoft/deberta-v3-base, your/custom/model)
  #  - embed_pipeline = pipeline the model comes from (i.e. huggingface, sentencetransformer, finetuned)
  #  - embed_type = which embedding to use (e.g. cls, mean, max for huggingface)
  #  - processor_output_dim = what is the size of the embedding we want after initial projection to shared feature space?

  experts:
    - embed_model: cardiffnlp/twitter-roberta-base-sentiment-latest
      embed_pipeline: huggingface
      embed_type: mean
      processor_output_dim: 512
    - embed_model: siebert/sentiment-roberta-large-english
      embed_pipeline: huggingface
      embed_type: mean
      processor_output_dim: 512
    - embed_model: FacebookAI/roberta-large
      embed_pipeline: huggingface
      embed_type: mean
      processor_output_dim: 512
    - embed_model: tabularisai/multilingual-sentiment-analysis
      embed_pipeline: huggingface
      embed_type: mean
      processor_output_dim: 512

  # training
  batch_size: 64
  num_epochs: 20
  learning_rate: 0.001
  weight_decay: 0.01
  patience: 5
  diversity_coeff: 0.01
  entropy_coeff: 0.03 # important for softmax gates to prevent gate collapse

seed: 0
deterministic: False

val_split_size: 0.1

pipeline:
  name: finetuned_classifier.FinetunedClassifier

  label_mapping:
    negative: 0
    neutral: 1
    positive: 2

  model:
    pretrained_model_name_or_path: cardiffnlp/twitter-roberta-base-sentiment-latest
    num_labels: 3

  # freeze:
  #   - roberta.embeddings
  #   - roberta.encoder.layer.0
  #   - roberta.encoder.layer.1
  #   - roberta.encoder.layer.2
  #   - roberta.encoder.layer.3
  #   - roberta.encoder.layer.4
  #   - roberta.encoder.layer.5
  #   - roberta.encoder.layer.6
  #   - roberta.encoder.layer.7
  #   - roberta.encoder.layer.8
  #   - roberta.encoder.layer.9
  #   - roberta.encoder.layer.10
  #   - roberta.encoder.layer.11

  # peft:
  #   r: 32
  #   lora_alpha: 64
  #   lora_dropout: 0.05
  #   bias: none
  #   task_type: SEQ_CLS

  preprocessing:
    batch_size: 32

    # difficulty_filter:
    #   path: /home/dayang/dev/workspaces/sentiment-analysis/output/sentiment_model_voting.csv
    #   score_name: score
    #   score_threshold: 0.5
    #   p: 0.5

    tokenizer:
      padding: true
      truncation: true
      max_length: 512

  trainer:
    seed: ${seed}

    # objective parameters
    class_weights: auto

    # training parameters
    per_device_train_batch_size: 16
    gradient_accumulation_steps: 2
    num_train_epochs: 2
    learning_rate: 1e-6
    lr_scheduler_type: constant

    # eval parameters
    per_device_eval_batch_size: 64
    eval_strategy: steps
    eval_steps: 0.02

    # logging parameters
    logging_strategy: steps
    logging_steps: 0.001

    # checkpointing parameters
    save_strategy: steps
    save_steps: 0.1
    save_total_limit: 2
    load_best_model_at_end: true
    metric_for_best_model: eval_score
